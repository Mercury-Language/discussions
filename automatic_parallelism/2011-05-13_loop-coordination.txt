# vim: ts=4 sw=4 et ft=text

Revisiting loop coordination
============================

This document has three parts:

    + Right recursive loops
    + Primatives
    + Left recursive loops

Right recursive loops
---------------------

Consider the right recursion transformation we already designed.
We will use the familiar map_foldl example, parallelised in the smart
way.  I have written it here without state variable notation.

map_foldl(_, _, [], Acc, Acc).
map_foldl(M, F, [X | Xs], Acc0, Acc),
    (
        M(X, Y),
        F(Y, Acc0, Acc1),
    &
        map_foldl(M, F, Xs, Acc1, Acc)
    ).

Here it is again after applying the dependant parallel conjunction
transformation so that we can see the future that was Acc1.  The
future is not pushed into the higher order call 'F'.  As we know
map_foldl will be duplicated and specialised so that the future can be
pushed into the recursive call.

map_foldl(_, _, [], Acc, Acc).
map_foldl(M, F, [X | Xs], Acc0, Acc) :-
    new_future(FutureAcc1),
    (
        M(X, Y),
        F(Y, Acc0, Acc1),
        signal(FutureAcc1, Acc1)
    &
        map_foldl_parallel(M, F, Xs, FutureAcc1, Acc)
    ).

map_foldl_parallel(_, _, [], FutureAcc0, Acc) :-
    wait(FutureAcc0, Acc).
map_foldl_parallel(M, F, [X | Xs], FutureAcc0, Acc) :-
    new_future(FutureAcc1),
    (
        M(X, Y),
        wait(FutureAcc0, Acc0),
        F(Y, Acc0, Acc1),
        signal(FutureAcc1, Acc1)
    &
        map_foldl_parallel(M, F, Xs, FutureAcc1, Acc)
    ).

Now, lets apply the right-recursive loop coordination transformation
that we're designing.  We'll consider the specialised form of
map_foldl (namely map_foldl_parallel) for two reasons:
    1) What to do in a non-specialised form of similar code is simple
       to deduce.
    2) This transformation doesn't apply to the original code since it
       no-longer contains a recursive call.

% Note that this new map_foldl_parallel should be inlined into map_foldl.
% We should probably do this in one step along with the dependant parallel
% conjunction pass.
map_foldl_parallel(M, F, L, FutureAcc0, Acc) :-
    % The compiler chooses an upperbound for the number of contexts to use,
    % here we use at most eight contexts.
    create_loop_control(8, LC),
    map_foldl_parallel_lc(LC, M, F, L, FutureAcc0, Acc).

map_foldl_parallel_lc(LC, _, _, [], FutureAcc0, Acc) :-
    % The base case.
    wait(FutureAcc0, Acc).
map_foldl_parallel_lc(LC, M, F, [X | Xs], FutureAcc0, Acc) :-
    new_future(FutureAcc1),
    (
        free_slot_in(LC, FS)
        % short_spark_queue
    ->
        spawn_off(FS, (
            M(X, Y),
            wait(FutureAcc0, Acc0),
            f(Y, Acc0, Acc1),
            signal(FutureAcc1, Acc1),
            join_and_terminate(FS, LC)
        ),
        map_foldl_parallel_lc(LC, M, F, Xs, FutureAcc1, Acc),
        % This barrier waits for the other job to complete, as it may
        % have written it's result to our stack (but it doesn't in
        % this example).
        join_and_continue(FS, LC)
    ;
        M(X, Y),
        wait(FutureAcc0, Acc0),
        f(Y, Acc0, Acc1),
        signal(FutureAcc1, Acc1),
        map_foldl_parallel_lc(LC, M, F, Xs, FutureAcc1, Acc)
    ).

We can improve on this by moving the barrier that the original thread executes
into the base case of the loop, where it will only be executed once.  This
barrier now waits for all the parallel tasks to finish before continuing, and
the thread that calls the barrier can also become a worker to work on some of
these tasks.  Note that this is not a new transformation, it's an improbment to
the above transformation.

map_foldl_parallel(M, F, L, FutureAcc0, Acc) :-
    % The compiler chooses a constant for the number of threads to
    % attempt to use, here we use eight threads.
    create_loop_control(8, LC),
    map_foldl_parallel_lc(LC, M, F, L, FutureAcc0, Acc).

map_foldl_parallel_lc(LC, _, _, [], FutureAcc0, Acc) :-
    % This is now the _only_ barrier that the original thread
    % executes.
    finish_loop(LC),

    % The base case.
    wait(FutureAcc0, Acc).
map_foldl_parallel_lc(LC, M, F, [X | Xs], FutureAcc0, Acc) :-
    new_future(FutureAcc1),
    (
        free_slot_in(LC, FS)
        % short_spark_queue
    ->
        spawn_off(FS, (
            M(X, Y),
            wait(FutureAcc0, Acc0),
            F(Y, Acc0, Acc1),
            signal(FutureAcc1, Acc1),
            join_and_terminate(FS, LC)
        ),
        map_foldl_parallel_lc(LC, M, F, Xs, FutureAcc1, Acc)
    ;
        M(X, Y),
        wait(FutureAcc0, Acc0),
        f(Y, Acc0, Acc1),
        signal(FutureAcc1, Acc1),
        map_foldl_parallel_lc(LC, M, F, Xs, FutureAcc1, Acc)
    ).

Now that the barrier has been moved, where it will only be executed once rather
than N times.  This makes the per-iteration cost lower.


Primitives
----------

The primative operations we need are:

    create_loop_control(Num, LC)
    finish_loop(LC)
    free_slot_in(LC, FS)
    spawn_off(FS, Code)
    join_and_terminate(FS, LC)

The loop control structure is:

    struct LoopControl {
        Slots       *slots;
        int         num_slots;
        int         outstanding_workers;
        Context     *waiting_context;
    }

    struct Slot {
        Context     *context;
        bool        is_free;
    }

The code for the primatives should be:

    create_loop_control(N, LC) {
        LC = malloc(...);
        LC->slots = malloc(...);
        for (i = 0; i < N, i++) {
            LC->slots[i].context = NULL;
            LC->slots[i].is_free = YES;
        }
        LC->num_slots = N;
        LC->outstanding_jobs = 0;
        LC->waiting_context = NULL;
    }

    finish_loop(LC) {
        if (LC->outstanding_jobs > 0) {
            /*
             * We have to wait, this context should sleep while we wait.
             */
            MR_ENGINE(MR_eng_xtxt)->MR_ctxt_resume = resume_label;
            LC->waiting_context = THIS_CONTEXT;
            MR_ENGINE(MR_eng_ctxt) = NULL;
            MR_idle(); /* this call does not return */
        } else {
resume_label:
            /*
             * At this point all the jobs are finished.
             */
            for (i = 0; i < LC->num_slots; i++) {
                if (LC->slots[i].context != NULL) {
                    release_context(LC->slots[i].context);
                }
            }
        }
    }

    free_slot_in(LC, FS) {
    }

    spawn_off{FS, CodePtr} {
    }

    join_and_terminate(LC, FS) {
    }

Left recursive loops can be transformed as follows
--------------------------------------------------

map_foldr(_, _, [], Acc, Acc).
map_foldr(M, F, [X | Xs], Acc0, Acc) :-
    (
        map_foldr(M, F, Xs, Acc0, Acc1),
    &
        M(X, Y),
        F(Y, Acc1, Acc)
    ).

Below is this code transformed for dependant parallel conjunctions.  Note that
the future is not pushed into any call, not even the recursive call because it
cannot be pushed over a call within the recursive call.  The compiler already
behaves like this.  Note also that Acc1 is duplicated and named-apart so that
each conjunct has a unique variable, the compiler also already handles this.
(Thanks Peter Wang).

map_foldr(_, _, [], Acc, Acc).
map_foldr(M, F, [X | Xs], Acc0, Acc) :-
    new_future(FutureAcc1),
    (
        map_foldr(M, F, Xs, Acc0, Acc1),
        signal(FutureAcc1, Acc1)
    &
        M(X, Y),
        wait(FutureAcc1, Acc1'),
        F(Y, Acc1', Acc)
    ).

Now transform this for loop coordination.

map_foldr(M, F, L, !Acc) :-
    Sparks = [],
    map_foldr_lc(LC, M, F, L, !Acc, !Sparks),
    % !.Sparks must be empty.
    assert(!.Sparks = []),
    finish_loop(LC).

% Note that the LC variable is an 'out' parameter.
:- mode map_foldr_lc(in, out, in, in, in, in, out) is det.

map_foldr_lc(LC, _, _, [], Acc, Acc, !Sparks) :-
    create_loop_control(8, LC),
    spawn_some_sparks(LC, !Sparks).
map_foldr_lc(M, F, [X | Xs], Acc0, Acc, !Sparks) :-
    new_future(FutureAcc1),
    Spark = new_spark((
        M(X, Y),
        wait(FutureAcc1, Acc1'),
        F(Y, Acc1', Acc)
    ),
    !:Sparks = [Spark | !.Sparks],
    map_foldr_lc(LC, M, F, Xs, Acc0, Acc1, !Sparks),
    signal(FutureAcc1, Acc1),
    spawn_some_sparks(LC, !Sparks),
    % We _need_ this barrier here so that we can return the value of
    % Acc which is computed by Spark.
    join_and_continue(LC, Spark).

spawn_sparks(LC, Sparks) :-
    impure_map(spawn(LC), Sparks).

Compared to the normal code the compiler generates this code executes
the sparks in the opposite order.  Which means they'll, on average,
block less often and for less time on the dependant variable.

In the above transformation the original thread executes one barrier
per iteration.  This is slower compared to right recursion shown
above.  The code that the compiler generates has a similar barrier, so
this is no worse.

There may be a possible optimisation, By realizing that the calls to F
must run in sequence but the calls to M may be parallelised I wonder
if we could split this into code that looked more like the following.
This is a completely different optimisation, but it would be very
powerful at avoiding synchronisation.

map_foldl(M, F, L0, !Acc) :-
    parallel_map(M, L0, L),
    foldl(F, L1, !Acc).


