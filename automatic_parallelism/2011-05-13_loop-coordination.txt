# vim: ts=4 sw=4 et ft=text

Revisiting loop coordination
----------------------------

Consider the right recursion transformation we already designed.
We will use the familiar map_foldl example, parallelised in the smart
way.  I have written it here without state variable notation.

map_foldl(_, _, [], Acc, Acc).
map_foldl(M, F, [X | Xs], Acc0, Acc),
    (
        M(X, Y),
        F(Y, Acc0, Acc1),
    &
        map_foldl(M, F, Xs, Acc1, Acc)
    ).

Here it is again after applying the dependant parallel conjunction
transformation so that we can see the future that was Acc1.  The
future is not pushed into the higher order call 'F'.  As we know
map_foldl will be duplicated and specialised so that the future can be
pushed into the recursive call.

map_foldl(_, _, [], Acc, Acc).
map_foldl(M, F, [X | Xs], Acc0, Acc) :-
    new_future(FutureAcc1),
    (
        M(X, Y),
        F(Y, Acc0, Acc1),
        signal(FutureAcc1, Acc1)
    &
        map_foldl_parallel(M, F, Xs, FutureAcc1, Acc)
    ).

map_foldl_parallel(_, _, [], FutureAcc0, Acc) :-
    wait(FutureAcc0, Acc).
map_foldl_parallel(M, F, [X | Xs], FutureAcc0, Acc) :-
    new_future(FutureAcc1),
    (
        M(X, Y),
        wait(FutureAcc0, Acc0),
        F(Y, Acc0, Acc1),
        signal(FutureAcc1, Acc1)
    &
        map_foldl_parallel(M, F, Xs, FutureAcc1, Acc)
    ).

Now, lets apply the right-recursive loop coordination transformation
that we're designing.  We'll consider the specialised form of
map_foldl for two reasons:
    1) What to do in a non-specialised form of similar code is simple
       to deduce.
    2) This transformation doesn't apply to the original code since it
       no-longer contains a recursive call.

map_foldl_parallel(M, F, L, FutureAcc0, Acc) :-
    % The compiler chooses a constant for the number of threads to
    % attempt to use, here we use eight threads.
    create_loop_control(8, LC),
    map_foldl_parallel_lc(LC, M, F, L, FutureAcc0, Acc).

map_foldl_parallel_lc(LC, _, _, [], FutureAcc0, Acc) :-
    % The base case.
    wait(FutureAcc0, Acc).
map_foldl_parallel_lc(LC, M, F, [X | Xs], FutureAcc0, Acc) :-
    new_future(FutureAcc1),
    (
        free_slot_in(LC, FS)
        % short_spark_queue
    ->
        spawn_off(FS, (
            M(X, Y),
            wait(FutureAcc0, Acc0),
            f(Y, Acc0, Acc1),
            signal(FutureAcc1, Acc1),
            join_and_terminate(FS, LC)
        ),
        map_foldl_parallel_lc(LC, M, F, Xs, FutureAcc1, Acc),
        % This barrier waits for the other job to complete, as it may
        % have written it's result to our stack (but it doesn't in
        % this example).
        join_and_continue(FS, LC)
    ;
        M(X, Y),
        wait(FutureAcc0, Acc0),
        f(Y, Acc0, Acc1),
        signal(FutureAcc1, Acc1),
        map_foldl_parallel_lc(LC, M, F, Xs, FutureAcc1, Acc)
    ).

We can improve on this by moving the barrier that the original thread
executes into the base case of the loop, where it will only be
executed once.  This barrier now waits for all the parallel tasks to
finish before continuing, and the thread that calls the barrier can
also become a worker to work on some of these tasks.

map_foldl_parallel(M, F, L, FutureAcc0, Acc) :-
    % The compiler chooses a constant for the number of threads to
    % attempt to use, here we use eight threads.
    create_loop_control(8, LC),
    map_foldl_parallel_lc(LC, M, F, L, FutureAcc0, Acc).

map_foldl_parallel_lc(LC, _, _, [], FutureAcc0, Acc) :-
    % This is now the _only_ barrier that the original thread
    % executes.
    finish_loop(LC),

    % The base case.
    wait(FutureAcc0, Acc).
map_foldl_parallel_lc(LC, M, F, [X | Xs], FutureAcc0, Acc) :-
    new_future(FutureAcc1),
    (
        free_slot_in(LC, FS)
        % short_spark_queue
    ->
        spawn_off(FS, (
            M(X, Y),
            wait(FutureAcc0, Acc0),
            F(Y, Acc0, Acc1),
            signal(FutureAcc1, Acc1),
            join_and_terminate(FS, LC)
        ),
        map_foldl_parallel_lc(LC, M, F, Xs, FutureAcc1, Acc)
    ;
        M(X, Y),
        wait(FutureAcc0, Acc0),
        f(Y, Acc0, Acc1),
        signal(FutureAcc1, Acc1),
        map_foldl_parallel_lc(LC, M, F, Xs, FutureAcc1, Acc)
    ).

Now that the barrier has been moved and the thread that creates all
the tasks becomes a worker when it executes the new barrier in the
base case the per-iteration cost is now lower.  The per-iteration cost
can be reduced further by giving each thread it's own queue of tasks.
This will reduce the amount of synchronisation needed by the threads
executing the tasks.  Each thread simply iterates over it's queue of
tasks, and the barrier at the end of each task can be implemented very
cheaply and made part of the loop over the queue.

The per-iteration cost may now be low enough that the free_slot_in
test is no-longer needed.  But this is something that we would need to
test.

I've avoided calling this test granularity control because it isn't,
(but it is close).  Granularity control would turn a loop with
millions of small tasks into a loop of hundreds of larger tasks.  If
the free_slot_in test above fails, the current task will simply be
executed by the current thread, if it fails twice in a row, then two
tasks will be executed in sequence by the current thread, which is
similar to granularity control but not as powerful.  Really
granularity control would allow us to sequence tasks like this in any
thread, not just the current one.

Lets try to transform the above code so that real granularity control
is done.

map_foldl_parallel(M, F, L, FutureAcc0, Acc) :-
    % The compiler chooses a constant for the number of threads to
    % attempt to use, here we use eight threads.
    create_loop_control(8, LC),
    map_foldl_parallel_lc(LC, M, F, L, FutureAcc0, Acc).

map_foldl_parallel_lc(LC, _, _, [], FutureAcc0, Acc) :-
    % This is now the _only_ barrier that the original thread
    % executes.
    finish_loop(LC),

    % The base case.
    wait(FutureAcc0, Acc).
map_foldl_parallel_lc(LC, M, F, [X | Xs], FutureAcc0, Acc) :-
    new_future(FutureAccGC),
    (
        free_slot_in(LC, FS)
        % short_spark_queue
    ->
        spawn_off(FS, (
            M(X, Y),
            wait(FutureAcc0, Acc0),
            F(Y, Acc0, Acc1),
            foldl_parallel_gc(20, M, F, Xs, Acc1, FutureAccGC),
            join_and_terminate(FS, LC)
        ),
        (
            % Move ahead in the list to the 20th item (in sync with
            % the call above)
            drop(20, Xs, XsGc)
        ->
            map_foldl_parallel_lc(LC, M, F, XsGC, FutureAccGC, Acc)
        ;
            % Execute the base case.
            map_foldl_parallel_lc(LC, M, F, [], FutureAccGC, Acc)
        )
    ;
        M(X, Y),
        wait(FutureAcc0, Acc0),
        f(Y, Acc0, Acc1),
        signal(FutureAcc1, Acc1),
        map_foldl_parallel_lc_gc(20, LC, M, F, Xs, FutureAcc1, Acc)
    ).

map_foldl_gc(_, _, _, [], Acc0, FutureAcc) :-
    signal(FutureAcc, Acc0).
map_foldl_gc(N, M, F, [X | Xs], Acc0, FutureAcc) :-
    ( N > 0 ->
        M(X, Y),
        F(Y, Acc0, Acc1),
        map_foldl_gc(N - 1, M, F, Xs, Acc1, FutureAcc)
    ;
        signal(FutureAcc, Acc0)
    ).

This would be very effective, but the example above only works when
iterating over a list.  There may be a more generic way to do this,
possibly by extracting the iteration code out of the original loop,
but it's probably a lot harder and not worth while doing immediately.
We can discuss any ideas about it though.  Or about other ways to
introduce true granularity control into loops like this.  Probably by
applying the granularity control transformation before applying
dependant conjunction and loop control transformations.


Left recursive loops can be transformed as follows
--------------------------------------------------

map_foldr(_, _, [], Acc, Acc).
map_foldr(M, F, [X | Xs], Acc0, Acc) :-
    (
        map_foldr(M, F, Xs, Acc0, Acc1),
    &
        M(X, Y),
        F(Y, Acc1, Acc)
    ).

Transformed for dependant parallel conjunctions.  Note that the future
is not pushed into any call, not even the recursive call because it
cannot be pushed over a call within the recursive call.  The compiler
already behaves like this.  Note also that Acc1 is duplicated and
named-apart so that each conjunct has a unique variable, the compiler
also already handles this.

map_foldr(_, _, [], Acc, Acc).
map_foldr(M, F, [X | Xs], Acc0, Acc) :-
    new_future(FutureAcc1),
    (
        map_foldr(M, F, Xs, Acc0, Acc1),
        signal(FutureAcc1, Acc1)
    &
        M(X, Y),
        wait(FutureAcc1, Acc1'),
        F(Y, Acc1', Acc)
    ).

Now transform this for loop coordination.

map_foldr(M, F, L, !Acc) :-
    Sparks = [],
    map_foldr_lc(Sparks, LC, M, F, L, !Acc),
    finish_loop(LC).

% Note that the LC variable is an 'out' parameter.
:- mode map_foldr_lc(in, out, in, in, in, in, out) is det.

map_foldr_lc(Sparks, LC, _, _, [], Acc, Acc) :-
    create_loop_control(8, LC),
    spawn_sparks(LC, Sparks).
map_foldr_lc(Sparks0, M, F, [X | Xs], Acc0, Acc) :-
    new_future(FutureAcc1),
    Spark = new_spark((
        M(X, Y),
        wait(FutureAcc1, Acc1'),
        F(Y, Acc1', Acc)
    ),
    Sparks = [Spark | Sparks0],
    map_foldr_lc(Sparks, LC, M, F, Xs, Acc0, Acc1),
    signal(FutureAcc1, Acc1),
    % We _need_ this barrier here so that we can return the value of
    % Acc which is computed by Spark.
    join_and_continue(LC, Spark).

spawn_sparks(LC, Sparks) :-
    impure_map(spawn(LC), Sparks).

Compared to the normal code the compiler generates this code executes
the sparks in the opposite order.  Which means they'll, on average,
block less often and for less time on the dependant variable.

In the above transformation the original thread executes one barrier
per iteration.  This is slower compared to right recursion shown
above.  The code that the compiler generates has a similar barrier, so
this is no worse.

There may be a possible optimisation, By realizing that the calls to F
must run in sequence but the calls to M may be parallelised I wonder
if we could split this into code that looked more like the following.
This is a completely different optimisation, but it would be very
powerful at avoiding synchronisation.

map_foldl(M, F, L0, !Acc) :-
    parallel_map(M, L0, L),
    foldl(F, L1, !Acc).


Primitives
----------

The loop control is:

struct LoopControl {
    SparkQuues **queues;
    int        num_queues;
    int        outstanding_workers;
    bool       finish;
}

The builtins should be:

create_loop_control(N, LC) {
    % Setup loop control structure.
    LC->queues = allocate_queues(N);
    LC->num_queues = N;
    LC->outstanding_workers = N;
    LC->finish = NO;

    % XXX: the nth job will be late, this won't balance well.
    % This balance could be corrected by executing every Nth
    % computation locally, but that delays the creation of every task
    % after that, reducing the parallelism by 1/N.
    % But the current solution already looses this much parallelism in
    % dependant code because each Nth task won't be entered until all
    % the tasks have been put on queues.
    % The only solution that solves the balance problem is one with a
    % single task queue, but that has more overhead.
    Fork n-1 workers (see worker()) to work on n-1 of the queues.
}

worker(LC, N) {
    do {
        task = get_task(LC->queues[N]);
        if (task) {
            MR_CALL(task);
            continue;
        } else if (LC->finish) {
            ATOMIC(LC->outstanding_workers--);
        } else {
            Suspend, and wakeup once our queue has some work.
        }
    }
}

spawn_off simply adds a task to a queue in a round-robin fashion.  It
also wakes sleeping workers, an array of workers should be kept in the
loop control structure and filled in for sleeping workers.  (Workers
ought to be contexts).

finish_loop(LC) {
    # Become the Nth worker.
    N = LC->num_queues;
    
    % Tell the other workers that there is no more work so that they 
    % exit.
    LC->finish = YES;
    wakeup_sleeping_workers();

    % Finish this worker's work.
    worker(LC, N);

    % Wait until all the work is done.
    SleepUntil(LC->outstanding_workers == 0);
}
