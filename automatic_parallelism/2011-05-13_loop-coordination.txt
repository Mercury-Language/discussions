# vim: ts=4 sw=4 et ft=text

Revisiting loop coordination
============================

This document has three parts:

    + Right recursive loops
    + Primatives
    + Left recursive loops

Right recursive loops
---------------------

Consider the right recursion transformation we already designed.
We will use the familiar map_foldl example, parallelised in the smart
way.  I have written it here without state variable notation.

map_foldl(_, _, [], Acc, Acc).
map_foldl(M, F, [X | Xs], Acc0, Acc),
    (
        M(X, Y),
        F(Y, Acc0, Acc1),
    &
        map_foldl(M, F, Xs, Acc1, Acc)
    ).

Here it is again after applying the dependant parallel conjunction
transformation so that we can see the future that was Acc1.  The
future is not pushed into the higher order call 'F'.  As we know
map_foldl will be duplicated and specialised so that the future can be
pushed into the recursive call.

map_foldl(_, _, [], Acc, Acc).
map_foldl(M, F, [X | Xs], Acc0, Acc) :-
    new_future(FutureAcc1),
    (
        M(X, Y),
        F(Y, Acc0, Acc1),
        signal(FutureAcc1, Acc1)
    &
        map_foldl_parallel(M, F, Xs, FutureAcc1, Acc)
    ).

map_foldl_parallel(_, _, [], FutureAcc0, Acc) :-
    wait(FutureAcc0, Acc).
map_foldl_parallel(M, F, [X | Xs], FutureAcc0, Acc) :-
    new_future(FutureAcc1),
    (
        M(X, Y),
        wait(FutureAcc0, Acc0),
        F(Y, Acc0, Acc1),
        signal(FutureAcc1, Acc1)
    &
        map_foldl_parallel(M, F, Xs, FutureAcc1, Acc)
    ).

Now, lets apply the right-recursive loop coordination transformation
that we're designing.  We'll consider the specialised form of
map_foldl (namely map_foldl_parallel) for two reasons:
    1) What to do in a non-specialised form of similar code is simple
       to deduce.
    2) This transformation doesn't apply to the original code since it
       no-longer contains a recursive call.

% Note that this new map_foldl_parallel should be inlined into map_foldl.
% We should probably do this in one step along with the dependant parallel
% conjunction pass.
map_foldl_parallel(M, F, L, FutureAcc0, Acc) :-
    % The compiler chooses an upperbound for the number of contexts to use,
    % here we use at most eight contexts.
    create_loop_control(8, LC),
    map_foldl_parallel_lc(LC, M, F, L, FutureAcc0, Acc).

map_foldl_parallel_lc(LC, _, _, [], FutureAcc0, Acc) :-
    % The base case.
    wait(FutureAcc0, Acc).
map_foldl_parallel_lc(LC, M, F, [X | Xs], FutureAcc0, Acc) :-
    new_future(FutureAcc1),
    (
        free_slot_in(LC, FS)
        % short_spark_queue
    ->
        spawn_off(FS, (
            M(X, Y),
            wait(FutureAcc0, Acc0),
            f(Y, Acc0, Acc1),
            signal(FutureAcc1, Acc1),
            join_and_terminate(FS, LC)
        ),
        map_foldl_parallel_lc(LC, M, F, Xs, FutureAcc1, Acc),
        % This barrier waits for the other job to complete, as it may
        % have written it's result to our stack (but it doesn't in
        % this example).
        join_and_continue(FS, LC)
    ;
        M(X, Y),
        wait(FutureAcc0, Acc0),
        f(Y, Acc0, Acc1),
        signal(FutureAcc1, Acc1),
        map_foldl_parallel_lc(LC, M, F, Xs, FutureAcc1, Acc)
    ).

We can improve on this by moving the barrier that the original thread executes
into the base case of the loop, where it will only be executed once.  This
barrier now waits for all the parallel tasks to finish before continuing, and
the thread that calls the barrier can also become a worker to work on some of
these tasks.  Note that this is not a new transformation, it's an improvement to
the above transformation.

map_foldl_parallel(M, F, L, FutureAcc0, Acc) :-
    % The compiler chooses a constant for the number of threads to
    % attempt to use, here we use eight threads.
    create_loop_control(8, LC),
    map_foldl_parallel_lc(LC, M, F, L, FutureAcc0, Acc).

map_foldl_parallel_lc(LC, _, _, [], FutureAcc0, Acc) :-
    % This is now the _only_ barrier that the original thread
    % executes.
    finish_loop(LC),

    % The base case.
    wait(FutureAcc0, Acc).
map_foldl_parallel_lc(LC, M, F, [X | Xs], FutureAcc0, Acc) :-
    new_future(FutureAcc1),
    (
        free_slot_in(LC, FS)
        % short_spark_queue
    ->
        spawn_off(FS, (
            M(X, Y),
            wait(FutureAcc0, Acc0),
            F(Y, Acc0, Acc1),
            signal(FutureAcc1, Acc1),
            join_and_terminate(FS, LC)
        ),
        map_foldl_parallel_lc(LC, M, F, Xs, FutureAcc1, Acc)
    ;
        M(X, Y),
        wait(FutureAcc0, Acc0),
        f(Y, Acc0, Acc1),
        signal(FutureAcc1, Acc1),
        map_foldl_parallel_lc(LC, M, F, Xs, FutureAcc1, Acc)
    ).

Now that the barrier has been moved, where it will only be executed once rather
than N times.  This makes the per-iteration cost lower.


Primitives
----------

The primative operations we need are:

    create_loop_control(Num, LC)
    finish_loop(LC)
    free_slot_in(LC, FS)
    spawn_off(FS, Code)
    join_and_terminate(FS, LC)

The loop control structure is:

    struct LoopControl {
        Slots       *slots;
        int         num_slots;
        int         outstanding_workers;
        Context     *waiting_context;
    }

    struct Slot {
        Context     *context;
        bool        is_free;
    }

The code for the primatives should be:

    create_loop_control(N, LC) {
        LC = malloc(...);
        LC->slots = malloc(...);
        for (i = 0; i < N, i++) {
            LC->slots[i].context = NULL;
            LC->slots[i].is_free = TRUE;
        }
        LC->num_slots = N;
        LC->outstanding_jobs = 0;
        LC->waiting_context = NULL;
    }

    finish_loop(LC) {
        if (LC->outstanding_jobs > 0) {
            /*
             * We have to wait, this context should sleep while we wait.
             */
            MR_ENGINE(MR_eng_ctxt)->MR_ctxt_resume = resume_label;
            LC->waiting_context = THIS_CONTEXT;
            MR_ENGINE(MR_eng_ctxt) = NULL;
            MR_idle(); /* this call does not return */
        } else {
resume_label:
            /*
             * At this point all the jobs are finished.
             */
            for (i = 0; i < LC->num_slots; i++) {
                if (LC->slots[i].context != NULL) {
                    release_context(LC->slots[i].context);
                }
            }
        }
    }

    free_slot_in(LC, FS) {
        FS = NULL;
        if (LC->outstanding_jobs == LC->num_slots) {
            /* In some cases we can trivially return NULL */
        } else {
            /*
             * We could potentially optimize this search by starting i at the
             * last place we looked last time we ran this loop.
             */
            for (i = 0; i < LC->num_slots; i++) {
                if (LC->slots[i].is_free == TRUE) {
                    FS = &(LC->slots[i]);
                    /*
                     * Note that we don't need to protect access to is_free,
                     * only one thread ever sets it to FALSE if it is TRUE (the
                     * original thread), and only one thread can ever set it to
                     * TRUE once it becomes false (the thread that uses the
                     * slot)
                     */
                    FS->is_free = FALSE;
                    atomic_increment(LC->outstanding_jobs);
                    break;
                }
            }
        }
    }

    spawn_off{FS, CodePtr} {
        if (FS->context != NULL) {
            FS->context->MR_ctxt_resume = CodePtr;
            /*
             * This either:
             *   1) Finds an idle engine and starts a context executing on it
             *      immediatly.
             *   2) Or, if there is no idle engine adds it to the global run
             *      queue.
             */
            MR_schedule_context(FS->context);
        } else {
            /*
             * No context has been allocated. and we'd rather not allocate one
             * using this thread if we can help it, because that will delay
             * other things this thread could do:
             * 1) If there is an idle engine we should pass it a spark
             *    representing CodePtr.
             * 2) If there is no idle engine then queue that spark, we will
             *    either execute it later or it will be stolen by an engine
             *    once one becomes idle.
             * Either way, we create and schedule a spark.  The spark will fill
             * in the FS->context before it terminates.
             *
             * We need to modify the runtime in two ways to support this.
             * 1) This spark is structuraly different to other sparks, it's not
             *    associated with a normal parallel conjunction barrier.
             * 2) The runtime dosn't yet support passing sparks directly to
             *    other engines, but it should.
             */
            create_and_schedule_spark(CodePtr);
        }
    }

    join_and_terminate(LC, FS) {
        atomic_decrement(LC->outstanding_jobs);
        FS->context = MR_ENGINE(MR_eng_context);
        MEMORY_WRITE_BARRIER; /* Make sure context is visable before is_free */
        FS->is_free = TRUE;

        if (LC->waiting_context)
        {
            /*
            ** THe loop is finished and the first has already hit it's final
            ** barrier.  We have to wake it up.
            */
            schedule_context(LC->waiting_context);
        }

        MR_ENGINE(MR_eng_context) = NULL;
        MR_idle();
    }

There are a number of optimizations here.

Firstly, rather than calling schedule_context to wakeup the original context an
engine can instead swtich to that context directly rather than going through
the runqueue and idle loop.

Secondly, a hint can be provided to free_slot_in either of the next unsearch
slot and/or a recently freed slot.  This may help in cases where there are a
number of slots and checking them all is wasefull.

Workers could be pinned to engines to prevent cache misses after aquireing new
work.  The cache misses arn't likly to occur for the heap but they _are_ likly
to occur for the stack.

Peter's concern 2011-08-15
--------------------------

These are preliminary benchmarks, (we should update these after making
optimizations):

    Independent parallelism:                                    ~3.9s       
    Dependent parallelism:                                      ~4.5s
                                                                     
    Dependent parallelism with loop control and 4 contexts:     ~6.2s
    Dependent parallelism with loop control and 8 contexts:     ~5.2s
    Dependent parallelism with loop control and 16 contexts:    ~4.6s
    Dependent parallelism with loop control and 32 contexts:    ~4.2s

In the previous week's meeting Peter was concerned that there may be many wait
states in this algorithm.  This can be caused by a case where free_slot_in
returns false and the original context executes work locally.  Then, while it
is working a slot becomes free but cannot retrieve new work of it's own, it must
wait until the original thread finishes it's work before spawning off new work.
This is mitigated by having more contexts than the number of CPUs, which
didn't work as well as we expected; we need somewhere between 16 and 32 CPUs
before we can beat the performance of dependant parallelism - which is more
than I (Paul) expected.

To avoid this problem the original context should go to sleep when there are no
free slots, and be woken up when a slot has no new work to go on with.
Furthermore, rather than being woken by being added to the runqueue it can be
ran directly by the engine whose context ran out of work.  A similar context
switch can be used to switch to workers.


Avoiding waits due to blocking futures
--------------------------------------

Zoltan and I both noticed that there is also a wait state caused by blocking on
futures.  Where code is of the form of map_foldl we suggest recognizing which
part of the code is dependant and which is independent.  (Possibly performing
swaps to make as much of it independent as possible).  Then, using loop control
a list-like structure is built, each node is something like.

:- lc_skel(T, U)
    --->    lci_end
    ;       lci_incomplete(
                T
                lvi_skel(T, U)
            )
    ;       lci_ready(
                U,
                lci_skel(T, U)
            ).

T and U are tuples, possibly flattened into this datastructure, T represents
the input to the dependant part of the algorithm, and U represents the output
from the independent part of the algorithm.

The main thread (the caller of the loop) iterates quickly creating
lci_incomplete nodes terminated by an lci_end node.  After creating the final
node it blocks.  Each iteration of this loop passes one of the list's elements
to a loop-control worker.  Workers turn these lci_incomplete nodes into
lci_ready nodes using the independent code from the loop.  Another context
(spawned by the original context) starts at the beginning of the list taking
the lci_ready nodes and performing the dependent part of the loop to update an
accumulator.  If it reaches an lci_incomplete nodes it sleeps until that node
(and possibly some more nodes along the list) is/are ready, and then wakes up
to continue.  When it finds the lci_end node it termites, communicating the
final value of the accumulator to the original context.

As nodes are consumed by the context performing the dependent part of the loop
they are no-longer referenced and become candidates for garbage collection.

Not described are ways to wake up sleeping contexts such as the context
consuming the nodes of the list.  These are easy to imagine.


Left recursive loops can be transformed as follows
--------------------------------------------------

map_foldr(_, _, [], Acc, Acc).
map_foldr(M, F, [X | Xs], Acc0, Acc) :-
    (
        map_foldr(M, F, Xs, Acc0, Acc1),
    &
        M(X, Y),
        F(Y, Acc1, Acc)
    ).

Below is this code transformed for dependant parallel conjunctions.  Note that
the future is not pushed into any call, not even the recursive call because it
cannot be pushed over a call within the recursive call.  The compiler already
behaves like this.  Note also that Acc1 is duplicated and named-apart so that
each conjunct has a unique variable, the compiler also already handles this.
(Thanks Peter Wang).

map_foldr(_, _, [], Acc, Acc).
map_foldr(M, F, [X | Xs], Acc0, Acc) :-
    new_future(FutureAcc1),
    (
        map_foldr(M, F, Xs, Acc0, Acc1),
        signal(FutureAcc1, Acc1)
    &
        M(X, Y),
        wait(FutureAcc1, Acc1'),
        F(Y, Acc1', Acc)
    ).

Now transform this for loop coordination.

map_foldr(M, F, L, !Acc) :-
    Sparks = [],
    map_foldr_lc(LC, M, F, L, !Acc, !Sparks),
    % !.Sparks must be empty.
    assert(!.Sparks = []),
    finish_loop(LC).

% Note that the LC variable is an 'out' parameter.
:- mode map_foldr_lc(out, in, in, in, in, out, in, out) is det.

map_foldr_lc(LC, _, _, [], Acc, Acc, !Sparks) :-
    create_loop_control(8, LC),
    spawn_some_sparks(LC, !Sparks).
map_foldr_lc(M, F, [X | Xs], Acc0, Acc, !Sparks) :-
    new_future(FutureAcc1),
    Spark = new_spark((
        M(X, Y),
        wait(FutureAcc1, Acc1'),
        F(Y, Acc1', Acc)
    ),
    !:Sparks = [Spark | !.Sparks],
    map_foldr_lc(LC, M, F, Xs, Acc0, Acc1, !Sparks),
    signal(FutureAcc1, Acc1),
    spawn_some_sparks(LC, !Sparks),
    % We _need_ this barrier here so that we can return the value of
    % Acc which is computed by Spark.
    join_and_continue(LC, Spark).

spawn_some_sparks(LC, !Sparks) :-
    (
        !.Sparks = []
    ;
        !.Sparks = [Spark | NewSparks],
        ( free_slot(LC, FS) ->
            spawn_spark(FS, Spark),
            !:Sparks = NewSparks,
            spawn_some_sparks(LC, !Sparks)
        ;
            true
        )
    ).

Compared to the normal code the compiler generates this code executes
the sparks in the opposite order.  Which means they'll, on average,
block less often and for less time on the dependant variable.

In the above transformation the original thread executes one barrier
per iteration.  This is slower compared to right recursion shown
above.  The code that the compiler generates has a similar barrier, so
this is no worse.

In the above code the signal of the future followed by the barrier is not all
that useful.  Since there's no code outside of the sparks that actually uses
the result of the recursive call.  Therefore we can move this barrier and move
the signal into the spark.

:- mode map_foldr_lc(out, in, in, in, in, in, in, out) is det.

map_foldr_lc(LC, _, _, [], !FutureAcc, !Sparks) :-
    create_loop_control(8, LC),
    spawn_some_sparks(LC, !Sparks).
map_foldr_lc(LC, M, F, [X | Xs], FutureAcc0, FutureAcc, !Sparks) :-
    new_future(FutureAcc1),
    Spark = new_spark(
        M(X, Y),
        wait(FutureAcc1, Acc1),
        F(Y, Acc1, Acc)
        signal(FutureAcc, Acc),
    ),
    !:Sparks = [Spark | !.Sparks],
    map_foldr_lc(LC, M, F, Xs, FutureAcc0, FutureAcc1, !Sparks),
    spawn_some_sparks(LC, !Sparks),
    % We _need_ this barrier here so that we can return the value of
    % Acc which is computed by Spark.
    join_and_continue(LC, Spark).



There may be a possible optimisation, By realizing that the calls to F
must run in sequence but the calls to M may be parallelised I wonder
if we could split this into code that looked more like the following.
This is a completely different optimisation, but it would be very
powerful at avoiding synchronisation.

map_foldl(M, F, L0, !Acc) :-
    parallel_map(M, L0, L),
    foldl(F, L, !Acc).

Or:

map_foldl(M, F, L0, !Acc) :-
    (
        parallel_map(M, L0, L)
    &
        foldl(F, L, !Acc)
    ).

We want to consider making L a stream, that is a futurelist.
That's covered in 2011-05-25_futures_in_data 


